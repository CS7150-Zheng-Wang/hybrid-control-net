{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 78.1k/78.1k [00:00<00:00, 743kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image downloaded successfully as 'sample_image.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "# URL of the image\n",
    "url = \"https://img.freepik.com/premium-photo/futuristic-cityscape-sunset-with-neon-lights_14117-901468.jpg\"\n",
    "\n",
    "# Download the image with progress bar\n",
    "response = requests.get(url, stream=True)\n",
    "response.raise_for_status()\n",
    "\n",
    "# Get file size for progress bar (if available)\n",
    "file_size = int(response.headers.get(\"content-length\", 0))\n",
    "\n",
    "# Write the file\n",
    "with open(\"sample_image.jpg\", \"wb\") as file:\n",
    "    if file_size:\n",
    "        with tqdm(\n",
    "            total=file_size, unit=\"B\", unit_scale=True, desc=\"Downloading\"\n",
    "        ) as pbar:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                file.write(chunk)\n",
    "                pbar.update(len(chunk))\n",
    "    else:\n",
    "        file.write(response.content)\n",
    "\n",
    "print(\"Image downloaded successfully as 'sample_image.jpg'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dc08f3b62964c57bebff492e6df3508",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import (\n",
    "    StableDiffusionControlNetPipeline,\n",
    "    ControlNetModel,\n",
    "    UniPCMultistepScheduler,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "from utils.controlnet import call_control_net\n",
    "\n",
    "StableDiffusionControlNetPipeline.__call__ = call_control_net\n",
    "StableDiffusionControlNetPipeline._callback_tensor_inputs = [\n",
    "    \"dt_latents\",\n",
    "    \"latents\",\n",
    "    \"prompt_embeds\",\n",
    "    \"negative_prompt_embeds\",\n",
    "]\n",
    "\n",
    "# 1. Load the pretrained ControlNet model (e.g. canny version) and the base Stable Diffusion model.\n",
    "controlnet = ControlNetModel.from_pretrained(\n",
    "    \"lllyasviel/sd-controlnet-canny\", torch_dtype=torch.float16, use_safetensors=True\n",
    ")\n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n",
    "    controlnet=controlnet,\n",
    "    torch_dtype=torch.float16,\n",
    "    use_safetensors=True,\n",
    ")\n",
    "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "pipe.enable_model_cpu_offload()  # Optional: useful if GPU memory is limited.\n",
    "\n",
    "# 2. Set parameters for the diffusion process.\n",
    "num_inference_steps = 50  # Total diffusion steps.\n",
    "visualize_dt_timestep = (\n",
    "    10  # Step at which to interrupt and capture the intermediate output.\n",
    ")\n",
    "\n",
    "\n",
    "# 3. Define a callback to interrupt the diffusion process at a specific step.\n",
    "def interrupt_callback(pipeline, i, t, callback_kwargs):\n",
    "    if i == visualize_dt_timestep:\n",
    "        pipeline._interrupt = True  # Interrupt the process at the specified step.\n",
    "    return callback_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'cv2' has no attribute 'cvtColor'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m sample_image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_image.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m image_cv \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(sample_image)\n\u001b[1;32m----> 7\u001b[0m image_cv \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m(image_cv, cv2\u001b[38;5;241m.\u001b[39mCOLOR_RGB2BGR)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Apply Canny edge detection (adjust thresholds as needed for your image)\u001b[39;00m\n\u001b[0;32m     10\u001b[0m low_threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'cv2' has no attribute 'cvtColor'"
     ]
    }
   ],
   "source": [
    "# 4. Provide your prompt and control input (e.g., a canny edge map as conditioning image).\n",
    "prompt = \"A futuristic cityscape at sunset\"\n",
    "# Convert the original image to a Canny edge map\n",
    "# Load the sample image\n",
    "sample_image = Image.open(\"sample_image.jpg\")\n",
    "image_cv = np.array(sample_image)\n",
    "image_cv = cv2.cvtColor(image_cv, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "# Apply Canny edge detection (adjust thresholds as needed for your image)\n",
    "low_threshold = 100\n",
    "high_threshold = 200\n",
    "edges = cv2.Canny(image_cv, low_threshold, high_threshold)\n",
    "\n",
    "# Convert edges to RGB for ControlNet input\n",
    "edges_rgb = cv2.cvtColor(edges, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "# Create the conditioning image from the edges\n",
    "conditioning_image = Image.fromarray(edges_rgb)\n",
    "\n",
    "# 5. Run the ControlNet pipeline with the callback.\n",
    "with torch.no_grad():\n",
    "    # The callback will stop the diffusion after the specified step.\n",
    "    intermediate_image = pipe(\n",
    "        prompt,\n",
    "        image=conditioning_image,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        callback_on_step_end=interrupt_callback,\n",
    "        callback_steps=1,  # Callback is called at every step.\n",
    "        generator=torch.manual_seed(42),  # Ensure reproducibility.\n",
    "    ).images[0]\n",
    "\n",
    "# 6. Visualize the intermediate output.\n",
    "plt.imshow(intermediate_image)\n",
    "plt.title(f\"Output at diffusion step {visualize_dt_timestep}\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS7150",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
